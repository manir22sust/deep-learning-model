{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Deep Learning Model to Predict Employee Retention Using Keras and TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Contents\n",
    "- Step 1: Data Pre-processing\n",
    "- Step 2: Separating Your Training and Testing Datasets\n",
    "- Step 3:Transforming the Data\n",
    "- Step 4: Building the Artificial Neural Network\n",
    "- Step 5: Running Predictions on the Test Set\n",
    "- Step 6: Checking the Confusion Matrix\n",
    "- Step 7: Making a Single Prediction\n",
    "- Step 8: Improving the Model Accuracy\n",
    "- Step 9: Adding Dropout Regularization to Fight Over-Fitting\n",
    "- Step 10: Hyperparameter Tuning\n",
    "- Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "- Keras is a neural network API that is written in Python. It runs on top of TensorFlow, CNTK, or Theano. It is a high-level abstraction of these deep learning frameworks and therefore makes experimentation faster and easier. Keras is modular, which means implementation is seamless as developers can quickly extend models by adding modules.\n",
    "\n",
    "- TensorFlow is an open-source software library for machine learning. It works efficiently with computation involving arrays; so it’s a great choice for the model you’ll build in this tutorial. Furthermore, TensorFlow allows for the execution of code on either CPU or GPU, which is a useful feature especially when you’re working with a massive dataset.\n",
    "\n",
    "- In this tutorial, you’ll build a deep learning model that will predict the probability of an employee leaving a company. Retaining the best employees is an important factor for most organizations. To build your model, you’ll use this dataset available at Kaggle https://www.kaggle.com/liujiaqi/hr-comma-sepcsv/downloads/hr-comma-sepcsv.zip/1, which has features that measure employee satisfaction in a company. To create this model, you’ll use the Keras sequential layer to build the different layers for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Pre-processing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Data Pre-processing is necessary to prepare your data in a manner that a deep learning model can accept. If there are categorical variables in your data, you have to convert them to numbers because the algorithm only accepts numerical figures. A categorical variable represents quantitive data represented by names. In this step, you’ll load in your dataset using pandas, which is a data manipulation Python library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, you’ll import the required modules for the project and then load the dataset in a notebook cell. You’ll load in the pandas module for manipulating your data and numpy for converting the data into numpy arrays. You’ll also convert all the columns that are in string format to numerical values for your computer to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_csv(\"HR_comma_sep.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can get a glimpse at the dataset you’re working with by using head(). This is a useful function from pandas that allows you to view the first five records of your dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>satisfaction_level</th>\n",
       "      <th>last_evaluation</th>\n",
       "      <th>number_project</th>\n",
       "      <th>average_montly_hours</th>\n",
       "      <th>time_spend_company</th>\n",
       "      <th>Work_accident</th>\n",
       "      <th>left</th>\n",
       "      <th>promotion_last_5years</th>\n",
       "      <th>sales</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2</td>\n",
       "      <td>157</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5</td>\n",
       "      <td>262</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.88</td>\n",
       "      <td>7</td>\n",
       "      <td>272</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.87</td>\n",
       "      <td>5</td>\n",
       "      <td>223</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.52</td>\n",
       "      <td>2</td>\n",
       "      <td>159</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   satisfaction_level  last_evaluation  number_project  average_montly_hours  \\\n",
       "0                0.38             0.53               2                   157   \n",
       "1                0.80             0.86               5                   262   \n",
       "2                0.11             0.88               7                   272   \n",
       "3                0.72             0.87               5                   223   \n",
       "4                0.37             0.52               2                   159   \n",
       "\n",
       "   time_spend_company  Work_accident  left  promotion_last_5years  sales  \\\n",
       "0                   3              0     1                      0  sales   \n",
       "1                   6              0     1                      0  sales   \n",
       "2                   4              0     1                      0  sales   \n",
       "3                   5              0     1                      0  sales   \n",
       "4                   3              0     1                      0  sales   \n",
       "\n",
       "   salary  \n",
       "0     low  \n",
       "1  medium  \n",
       "2  medium  \n",
       "3     low  \n",
       "4     low  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ll now proceed to convert the categorical columns to numbers. You do this by converting them to dummy variables. Dummy variables are usually ones and zeros that indicate the presence or absence of a categorical feature. In this kind of situation, you also avoid the dummy variable trap by dropping the first dummy."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Note: The dummy variable trap is a situation whereby two or more variables are highly correlated. This leads to your model performing poorly. You, therefore, drop one dummy variable to always remain with N-1 dummy variables. Any of the dummy variables can be dropped because there is no preference as long as you remain with N-1 dummy variables. An example of this is if you were to have an on/off switch. When you create the dummy variable you shall get two columns: an on column and an off column. You can drop one of the columns because if the switch isn’t on, then it is off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats=['sales','salary']\n",
    "df_final=pd.get_dummies(df,columns=feats,drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feats = ['sales','salary'] defines the two columns for which you want to create dummy variables. pd.get_dummies(df,columns=feats,drop_first=True) will generate the numerical variables that your employee retention model requires. It does this by converting the feats that you define from categorical to numerical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ve loaded in the dataset and converted the salary and department columns into a format the keras deep learning model can accept. In the next step, you will split the dataset into a training and testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Separating Your Training and Testing Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You’ll use scikit-learn to split your dataset into a training and a testing set. This is necessary so you can use part of the employee data to train the model and a part of it to test its performance. Splitting a dataset in this way is a common practice when building deep learning models.\n",
    "\n",
    "It is important to implement this split in the dataset so the model you build doesn’t have access to the testing data during the training process. This ensures that the model learns only from the training data, and you can then test its performance with the testing data. If you exposed your model to testing data during the training process then it would memorize the expected outcomes. Consequently, it would fail to give accurate predictions on data that it hasn’t seen.\n",
    "\n",
    "You’ll start by importing the train_test_split module from the scikit-learn package. This is the module that will provide the splitting functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the train_test_split module imported, you’ll use the left column in your dataset to predict if an employee will leave the company. Therefore, it is essential that your deep learning model doesn’t come into contact with this column. Insert the following into a cell to drop the left column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df_final.drop(['left'],axis=1).values\n",
    "y=df_final['left'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.38, 0.53, 2.  , ..., 0.  , 1.  , 0.  ],\n",
       "       [0.8 , 0.86, 5.  , ..., 0.  , 0.  , 1.  ],\n",
       "       [0.11, 0.88, 7.  , ..., 0.  , 0.  , 1.  ],\n",
       "       ...,\n",
       "       [0.37, 0.53, 2.  , ..., 0.  , 1.  , 0.  ],\n",
       "       [0.11, 0.96, 6.  , ..., 0.  , 1.  , 0.  ],\n",
       "       [0.37, 0.52, 2.  , ..., 0.  , 1.  , 0.  ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your deep learning model expects to get the data as arrays. Therefore you use numpy to convert the data to numpy arrays with the .values attribute.\n",
    "\n",
    "You’re now ready to convert the dataset into a testing and training set. You’ll use 70% of the data for training and 30% for testing. The training ratio is more than the testing ratio because you’ll need to use most of the data for the training process. If desired, you can also experiment with a ratio of 80% for the training set and 20% for the testing set.\n",
    "\n",
    "Now add this code to the next cell and run to split your training and testing data to the specified ratio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now converted the data into the type that Keras expects it to be in (numpy arrays), and your data is split into a training and testing set. You’ll pass this data to the keras model later in the tutorial. Beforehand you need to transform the data, which you’ll complete in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Transforming the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building deep learning models it is usually good practice to scale your dataset in order to make the computations more efficient. In this step, you’ll scale the data using the StandardScaler; this will ensure that your dataset values have a mean of zero and a unit variable. This transforms the dataset to be normally distributed. You’ll use the scikit-learn StandardScaler to scale the features to be within the same range. This will transform the values to have a mean of 0 and a standard deviation of 1. This step is important because you’re comparing features that have different measurements; so it is typically required in machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you start by importing the StandardScaler and calling an instance of it. You then use its fit_transform method to scale the training and testing set.\n",
    "\n",
    "You have scaled all your dataset features to be within the same range. You can start building the artificial neural network in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Building the Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now you will use keras to build the deep learning model. To do this, you’ll import keras, which will use tensorflow as the backend by default. From keras, you’ll then import the Sequential module to initialize the artificial neural network. An artificial neural network is a computational model that is built using inspiration from the workings of the human brain. You’ll import the Dense module as well, which will add layers to your deep learning model.\n",
    "\n",
    "When building a deep learning model you usually specify three layer types:\n",
    "\n",
    "- The input layer is the layer to which you’ll pass the features of your dataset. There is no computation that occurs in this layer. It serves to pass features to the hidden layers.\n",
    "- The hidden layers are usually the layers between the input layer and the output layer and there can be more than one. These layers perform the computations and pass the information to the output layer.\n",
    "- The output layer represents the layer of your neural network that will give you the results after training your model. It is responsible for producing the output variables.\n",
    "\n",
    "To import the Keras, Sequential, and Dense modules, run the following code in your notebook cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ll use Sequential to initialize a linear stack of layers. Since this is a classification problem, you’ll create a classifier variable. A classification problem is a task where you have labeled data and would like to make some predictions based on the labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier=Sequential()\n",
    "# You’ve used Sequential to initialize the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(9,kernel_initializer=\"uniform\",activation=\"relu\",input_dim=18))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You add layers using the .add() function on your classifier and specify some parameters:\n",
    "\n",
    "- The first parameter is the number of nodes that your network should have. The connection between different nodes is what forms the neural network. One of the strategies to determine the number of nodes is to take the average of the nodes in the input layer and the output layer.\n",
    "\n",
    "- The second parameter is the kernel_initializer. When you fit your deep learning model the weights will be initialized to numbers close to zero, but not zero. To achieve this you use the uniform distribution initializer. kernel_initializer is the function that initializes the weights.\n",
    "\n",
    "- The third parameter is the activation function. Your deep learning model will learn through this function. There are usually linear and non-linear activation functions. You use the relu activation function because it generalizes well on your data. Linear functions are not good for problems like these because they form a straight line.\n",
    "\n",
    "- The last parameter is input_dim, which represents the number of features in your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you’ll add the output layer that will give you the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(1,kernel_initializer=\"uniform\",activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output layer takes the following parameters:\n",
    "\n",
    "- The number of output nodes. You expect to get one output: if an employee leaves the company. Therefore you specify one output node.\n",
    "- For kernel_initializer you use the sigmoid activation function so that you can get the probability that an employee will leave. In the event that you were dealing with more than two categories, you would use the softmax activation function, which is a variant of the sigmoid activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you’ll apply a gradient descent to the neural network. This is an optimization strategy that works to reduce errors during the training process. Gradient descent is how randomly assigned weights in a neural network are adjusted by reducing the cost function, which is a measure of how well a neural network performs based on the output expected from it.\n",
    "\n",
    "The aim of a gradient descent is to get the point where the error is at its least. This is done by finding where the cost function is at its minimum, which is referred to as a local minimum. In gradient descent, you differentiate to find the slope at a specific point and find out if the slope is negative or positive—you’re descending into the minimum of the cost function. There are several types of optimization strategies, but you’ll use a popular one known as adam in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying gradient descent is done via the compile function that takes the following parameters:\n",
    "\n",
    "- optimizer is the gradient descent.\n",
    "- loss is a function that you’ll use in the gradient descent. Since this is a binary classification problem you use the binary_crossentropy loss function.\n",
    "- The last parameter is the metric that you’ll use to evaluate your model. In this case, you’d like to evaluate it based on its accuracy when making predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’re ready to fit your classifier to your dataset. Keras makes this possible via the .fit() method. To do this, insert the following code into your notebook and run it in order to fit the model to your dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "10499/10499 [==============================] - 1s 119us/step - loss: 0.4123 - acc: 0.8284\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x629aaec860>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train, batch_size = 10, epochs = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .fit() method takes a couple of parameters:\n",
    "\n",
    "- The first parameter is the training set with the features.\n",
    "- The second parameter is the column that you’re making the predictions on.\n",
    "- The batch_size represents the number of samples that will go through the neural network at each training round.\n",
    "- epochs represents the number of times that the dataset will be passed via the neural network. The more epochs the longer it will take to run your model, which also gives you better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ve created your deep learning model, compiled it, and fitted it to your dataset. You’re ready to make some predictions using the deep learning model. In the next step, you’ll start making predictions with the dataset that the model hasn’t yet seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Running Predictions on the Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start making predictions, you’ll use the testing dataset in the model that you’ve created. Keras enables you to make predictions by using the .predict() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you’ve already trained the classifier with the training set, this code will use the learning from the training process to make predictions on the test set. This will give you the probabilities of an employee leaving. You’ll work with a probability of 50% and above to indicate a high chance of the employee leaving the company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=(y_pred>0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ve created predictions using the predict method and set the threshold for determining if an employee is likely to leave. To evaluate how well the model performed on the predictions, you will next use a confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 6: Checking the Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, you will use a confusion matrix to check the number of correct and incorrect predictions. A confusion matrix, also known as an error matrix, is a square matrix that reports the number of true positives(tp), false positives(fp), true negatives(tn), and false negatives(fn) of a classifier.\n",
    "\n",
    "- A true positive is an outcome where the model correctly predicts the positive class (also known as sensitivity or recall).\n",
    "- A true negative is an outcome where the model correctly predicts the negative class.\n",
    "- A false positive is an outcome where the model incorrectly predicts the positive class.\n",
    "- A false negative is an outcome where the model incorrectly predicts the negative class.\n",
    "\n",
    "To achieve this you’ll use a confusion matrix that scikit-learn provides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3228,  199],\n",
       "       [ 211,  862]], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm=confusion_matrix(y_test,y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix output means that your deep learning model made 3228 + 862 correct predictions and 199+ 211 wrong predictions. You can calculate the accuracy with: (3228 + 862) / 4500. The total number of observations in your dataset is 4500. This gives you an accuracy of 90.89%. This is a very good accuracy rate since you can achieve at least 90% correct predictions from your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ve evaluated your model using the confusion matrix. Next, you’ll work on making a single prediction using the model that you have developed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Making a Single Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step you’ll make a single prediction given the details of one employee with your model. You will achieve this by predicting the probability of a single employee leaving the company. You’ll pass this employee’s features to the predict method. As you did earlier, you’ll scale the features as well and convert them to a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pred = classifier.predict(sc.transform(np.array([[0.26,0.7 ,3., 238., 6., 0.,0.,0.,0., 0.,0.,0.,0.,0.,1.,0., 0.,1.]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These features represent the features of a single employee. As shown in the dataset in step 1, these features represent: satisfaction level, last evaluation, number of projects, and so on. As you did in step 3, you have to transform the features in a manner that the deep learning model can accept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_pred = (new_pred > 0.5)\n",
    "new_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might decide to set a lower or higher threshold for your model. For example, you can set the threshold to be 60%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_pred = (new_pred > 0.6)\n",
    "new_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, you have seen how to make a single prediction given the features of a single employee. In the next step, you will work on improving the accuracy of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Improving the Model Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you train your model many times you’ll keep getting different results. The accuracies for each training have a high variance. In order to solve this problem, you’ll use K-fold cross-validation. Usually, K is set to 10. In this technique, the model is trained on the first 9 folds and tested on the last fold. This iteration continues until all folds have been used. Each of the iterations gives its own accuracy. The accuracy of the model becomes the average of all these accuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras enables you to implement K-fold cross-validation via the KerasClassifier wrapper. This wrapper is from scikit-learn cross-validation. You’ll start by importing the cross_val_score cross-validation function and the KerasClassifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_classifier():\n",
    "    classifier=Sequential()\n",
    "    classifier.add(Dense(9,kernel_initializer=\"uniform\",activation=\"relu\",input_dim=18))\n",
    "    classifier.add(Dense(1,kernel_initializer=\"uniform\",activation=\"sigmoid\"))\n",
    "    classifier.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you create a function that you’ll pass to the KerasClassifier the function is one of the arguments that the classifier expects. The function is a wrapper of the neural network design that you used earlier. The passed parameters are also similar to the ones used earlier in the tutorial. In the function, you first initialize the classifier using Sequential(), you then use Dense to add the input and output layer. Finally, you compile the classifier and return it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier=KerasClassifier(build_fn=make_classifier,batch_size=10,nb_epoch=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KerasClassifier takes three arguments:\n",
    "\n",
    "- build_fn: the function with the neural network design\n",
    "- batch_size: the number of samples to be passed via the network in each iteration\n",
    "- nb_epoch: the number of epochs the network will run\n",
    "\n",
    "Next, you apply the cross-validation using Scikit-learn’s cross_val_score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies=cross_val_score(estimator=classifier,X=X_train,y=y_train,cv=10,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will give you ten accuracies since you have specified the number of folds as 10. Therefore, you assign it to the accuracies variable and later use it to compute the mean accuracy. \n",
    "\n",
    "It takes the following arguments:\n",
    "- estimator: the classifier that you’ve just defined\n",
    "- X: the training set features\n",
    "- y: the value to be predicted in the training set\n",
    "- cv: the number of folds\n",
    "- n_jobs: the number of CPUs to use (specifying it as -1 will make use of all the available CPUs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have applied the cross-validation, you can compute the mean and variance of the accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.845220839969959"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean=accuracies.mean()\n",
    "mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your output you’ll see that the mean is 84%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0007601351998612877"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variance = accuracies.var()\n",
    "variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that the variance is 0.00076. Since the variance is very low, it means that your model is performing very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ve improved your model’s accuracy by using K-Fold cross-validation. In the next step, you’ll work on the overfitting problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9 : Adding Dropout Regularization to Fight Over-Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictive models are prone to a problem known as overfitting. This is a scenario whereby the model memorizes the results in the training set and isn’t able to generalize on data that it hasn’t seen. Typically you observe overfitting when you have a very high variance on accuracies. To help fight over-fitting in your model, you will add a layer to your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In neural networks, dropout regularization is the technique that fights overfitting by adding a Dropout layer in your neural network. It has a rate parameter that indicates the number of neurons that will deactivate at each iteration. The process of deactivating nerurons is usually random. In this case, you specify 0.1 as the rate meaning that 1% of the neurons will deactivate during the training process. The network design remains the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "classifier=Sequential()\n",
    "classifier.add(Dense(9,kernel_initializer=\"uniform\",activation=\"relu\",input_dim=18))\n",
    "classifier.add(Dropout(rate=0.1))\n",
    "classifier.add(Dense(1,kernel_initializer=\"uniform\",activation=\"sigmoid\"))\n",
    "classifier.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have added a Dropout layer between the input and output layer. Having set a dropout rate of 0.1 means that during the training process 15 of the neurons will deactivate so that the classifier doesn’t overfit on the training set. After adding the Dropout and output layers you then compiled the classifier as you have done previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You worked to fight over-fitting in this step with a Dropout layer. Next, you’ll work on further improving the model by tuning the parameters you used while creating the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10 : Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search is a technique that you can use to experiment with different model parameters in order to obtain the ones that give you the best accuracy. The technique does this by trying different parameters and returning those that give the best results. You’ll use grid search to search for the best parameters for your deep learning model. This will help in improving model accuracy. scikit-learn provides the GridSearchCV function to enable this functionality. You will now proceed to modify the make_classifier function to try out different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify the make_classifier function so you can test out different optimizer functions:\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "def make_classifier(optimizer):\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(9, kernel_initializer = \"uniform\", activation = \"relu\", input_dim=18))\n",
    "    classifier.add(Dense(1, kernel_initializer = \"uniform\", activation = \"sigmoid\"))\n",
    "    classifier.compile(optimizer= optimizer,loss = \"binary_crossentropy\",metrics = [\"accuracy\"])\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have started by importing GridSearchCV. You have then made changes to the make_classifier function so that you can try different optimizers. You’ve initialized the classifier, added the input and output layer, and then compiled the classifier. Finally, you have returned the classifier so you can use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Like in step 4, insert this line of code to define the classifier:\n",
    "classifier = KerasClassifier(build_fn = make_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ve defined the classifier using the KerasClassifier, which expects a function through the build_fn parameter. You have called the KerasClassifier and passed the make_classifier function that you created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will now proceed to set a couple of parameters that you wish to experiment with.\n",
    "\n",
    "\n",
    "params = {\n",
    "    'batch_size':[20,35],\n",
    "    'epochs':[2,3],\n",
    "    'optimizer':['adam','rmsprop']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you have added different batch sizes, number of epochs, and different types of optimizer functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a small dataset like yours, a batch size of between 20–35 is good. For large datasets its important to experiment with larger batch sizes. Using low numbers for the number of epochs ensures that you get results within a short period. However, you can experiment with bigger numbers that will take a while to complete depending on the processing speed of your server. The adam and rmsprop optimizers from keras are a good choice for this type of neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you’re going to use the different parameters you have defined to search for the best parameters using the GridSearchCV function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator=classifier,\n",
    "                           param_grid=params,\n",
    "                           scoring=\"accuracy\",\n",
    "                           cv=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid search function expects the following parameters:\n",
    "\n",
    "- estimator: the classifier that you’re using.\n",
    "- param_grid: the set of parameters that you’re going to test.\n",
    "- scoring: the metric you’re using.\n",
    "- cv: the number of folds you’ll test on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "5249/5249 [==============================] - 1s 107us/step - loss: 0.5700 - acc: 0.7640\n",
      "Epoch 2/2\n",
      "5249/5249 [==============================] - 0s 53us/step - loss: 0.4027 - acc: 0.7893\n",
      "Epoch 1/2\n",
      "5250/5250 [==============================] - 1s 106us/step - loss: 0.5738 - acc: 0.7589\n",
      "Epoch 2/2\n",
      "5250/5250 [==============================] - 0s 57us/step - loss: 0.4140 - acc: 0.7600\n",
      "Epoch 1/2\n",
      "5249/5249 [==============================] - 1s 104us/step - loss: 0.5845 - acc: 0.7666\n",
      "Epoch 2/2\n",
      "5249/5249 [==============================] - 0s 54us/step - loss: 0.4223 - acc: 0.8055\n",
      "Epoch 1/2\n",
      "5250/5250 [==============================] - 1s 110us/step - loss: 0.5823 - acc: 0.7592\n",
      "Epoch 2/2\n",
      "5250/5250 [==============================] - 0s 61us/step - loss: 0.4353 - acc: 0.7600\n",
      "Epoch 1/3\n",
      "5249/5249 [==============================] - 1s 129us/step - loss: 0.5777 - acc: 0.7630\n",
      "Epoch 2/3\n",
      "5249/5249 [==============================] - 0s 58us/step - loss: 0.4157 - acc: 0.7641\n",
      "Epoch 3/3\n",
      "5249/5249 [==============================] - 0s 54us/step - loss: 0.3765 - acc: 0.7641\n",
      "Epoch 1/3\n",
      "5250/5250 [==============================] - 1s 131us/step - loss: 0.5770 - acc: 0.7581\n",
      "Epoch 2/3\n",
      "5250/5250 [==============================] - 0s 61us/step - loss: 0.4239 - acc: 0.7600\n",
      "Epoch 3/3\n",
      "5250/5250 [==============================] - 0s 60us/step - loss: 0.3786 - acc: 0.7600\n",
      "Epoch 1/3\n",
      "5249/5249 [==============================] - 1s 127us/step - loss: 0.5932 - acc: 0.7630\n",
      "Epoch 2/3\n",
      "5249/5249 [==============================] - 0s 57us/step - loss: 0.4298 - acc: 0.8040\n",
      "Epoch 3/3\n",
      "5249/5249 [==============================] - 0s 55us/step - loss: 0.3613 - acc: 0.8118\n",
      "Epoch 1/3\n",
      "5250/5250 [==============================] - 1s 133us/step - loss: 0.5778 - acc: 0.7596\n",
      "Epoch 2/3\n",
      "5250/5250 [==============================] - 0s 59us/step - loss: 0.4285 - acc: 0.7600\n",
      "Epoch 3/3\n",
      "5250/5250 [==============================] - 0s 58us/step - loss: 0.3814 - acc: 0.7600\n",
      "Epoch 1/2\n",
      "5249/5249 [==============================] - 1s 119us/step - loss: 0.6320 - acc: 0.7565\n",
      "Epoch 2/2\n",
      "5249/5249 [==============================] - 0s 38us/step - loss: 0.4638 - acc: 0.7849\n",
      "Epoch 1/2\n",
      "5250/5250 [==============================] - 1s 127us/step - loss: 0.6508 - acc: 0.7537\n",
      "Epoch 2/2\n",
      "5250/5250 [==============================] - 0s 37us/step - loss: 0.5023 - acc: 0.8040\n",
      "Epoch 1/2\n",
      "5249/5249 [==============================] - 1s 127us/step - loss: 0.6300 - acc: 0.7630\n",
      "Epoch 2/2\n",
      "5249/5249 [==============================] - 0s 40us/step - loss: 0.4899 - acc: 0.7641\n",
      "Epoch 1/2\n",
      "5250/5250 [==============================] - 1s 132us/step - loss: 0.6385 - acc: 0.7573\n",
      "Epoch 2/2\n",
      "5250/5250 [==============================] - 0s 38us/step - loss: 0.5122 - acc: 0.7926\n",
      "Epoch 1/3\n",
      "5249/5249 [==============================] - 1s 141us/step - loss: 0.6506 - acc: 0.7474\n",
      "Epoch 2/3\n",
      "5249/5249 [==============================] - 0s 41us/step - loss: 0.4972 - acc: 0.8038\n",
      "Epoch 3/3\n",
      "5249/5249 [==============================] - 0s 38us/step - loss: 0.3888 - acc: 0.8312\n",
      "Epoch 1/3\n",
      "5250/5250 [==============================] - 1s 146us/step - loss: 0.6364 - acc: 0.7590\n",
      "Epoch 2/3\n",
      "5250/5250 [==============================] - 0s 39us/step - loss: 0.4788 - acc: 0.7916\n",
      "Epoch 3/3\n",
      "5250/5250 [==============================] - 0s 41us/step - loss: 0.3907 - acc: 0.7990\n",
      "Epoch 1/3\n",
      "5249/5249 [==============================] - 1s 144us/step - loss: 0.6340 - acc: 0.7613\n",
      "Epoch 2/3\n",
      "5249/5249 [==============================] - 0s 38us/step - loss: 0.4996 - acc: 0.7864\n",
      "Epoch 3/3\n",
      "5249/5249 [==============================] - 0s 39us/step - loss: 0.4056 - acc: 0.8106\n",
      "Epoch 1/3\n",
      "5250/5250 [==============================] - 1s 147us/step - loss: 0.6267 - acc: 0.7598\n",
      "Epoch 2/3\n",
      "5250/5250 [==============================] - 0s 44us/step - loss: 0.4887 - acc: 0.7600\n",
      "Epoch 3/3\n",
      "5250/5250 [==============================] - 0s 44us/step - loss: 0.4268 - acc: 0.7600\n",
      "Epoch 1/3\n",
      "10499/10499 [==============================] - 1s 101us/step - loss: 0.5542 - acc: 0.7602\n",
      "Epoch 2/3\n",
      "10499/10499 [==============================] - 0s 43us/step - loss: 0.3879 - acc: 0.8074\n",
      "Epoch 3/3\n",
      "10499/10499 [==============================] - 0s 40us/step - loss: 0.3203 - acc: 0.8611\n"
     ]
    }
   ],
   "source": [
    "# Next, you fit this grid_search to your training dataset:\n",
    "grid_search = grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the best parameters from this search using the best_params_ attribute:\n",
    "\n",
    "best_param = grid_search.best_params_\n",
    "best_accuracy = grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 35, 'epochs': 3, 'optimizer': 'adam'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output shows that the best batch size is 35, the best number of epochs is 3, and the adam optimizer is the best for your model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the best accuracy for your model. The best_accuracy number represents the highest accuracy you obtain from the best parameters after running the grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.827888370320983"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ve used GridSearch to figure out the best parameters for your classifier. You have seen that the best batch_size is 20, the best optimizer is the adam optimizer and the best number of epochs is 3. You have also obtained the best accuracy for your classifier as being 85%. You’ve built an employee retention model that is able to predict if an employee stays or leaves with an accuracy of up to 85%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you’ve used Keras to build an artificial neural network that predicts the probability that an employee will leave a company. You combined your previous knowledge in machine learning using scikit-learn to achieve this. To further improve your model, you can try different activation functions or optimizer functions from keras. You could also experiment with a different number of folds, or, even build a model with a different dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
